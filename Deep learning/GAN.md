# GAN (by Goodfellow 2014)

reference
[github repo/reimplementation](https://github.com/h3lio5/gan-pytorch](https://github.com/h3lio5/gan-pytorch)
[github repo/goodfellow](https://github.com/goodfeli/adversarial)

1. introduction 

- instead of training one model, we train two models where a generative model *G* capture data distribution and a discriminative model *D* estimates the probability of a sample of being generated by *G*
- the goal for *G* is to maximize the probablity of *D* misclassifying / classify wrongly in a [minimax fashion](../Game theory/Minimax.md)
- in the space of arbitary function *G* and *D* there exists a unique solution where *G* finds the distribution of the training data and *D* equals to 0.5 everywhere
- no [markov chains] or inference is needed during training and generating samples

in DL field discriminator finds more application (ie classification), where *D* have been primarily based on [backpropagation] and [dropout] algorithms.
Deep *generative* models sees less application due to the difficulties in approximating intractable probabilistic computation that arises in [maximum likelyhood estimation] and related strategies, and difficulty in leveraging on [piecewise linear units] in generative context (unlike *D*)

in Goodfellow 2014, they exploring a special case where both *D* and *G* are multilayer [perceptrons] by training both models using only highly successful backprop & dropout algorithms and sample from the generative model uning only forward propagation (? does this means that no backprop on *G*?)

2. related works

VAE
deep boltzman machines, restricted boltzman machines with estimation by markov chain monte carlo methods
deep belief networks
noise-contrastive estimation

3. Adversarial nets theory

```tex
\\(begin{equation}1+2=3\end{equation}\\)
```

```python
def func():
    pass
```
